---
layout: post
title: Nonogen
excerpt: "My attempt at making dynamic nonogram levels."
modified: 
categories: game-programming
tags: [unity, csharp]
comments: true
share: true
---

My girlfriend and I have both recently gotten into solving [Nonograms](https://www.nonograms.org/). At the same time I had started exploring Unity and game development in Unity. And the confluence of these events made me wonder how I could generate these Nonograms levels from a user-submitted image.

To that end, I started building a little prototype in Unity with Step 1 being able to load an image.

<img width="269" alt="image" src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/f3313591-39b9-4ed6-a629-16cbecfdf709"><img width="270" alt="image" src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/52441939-3fa4-4052-b627-14dcce76685b">

Next I needed a way to pixelize the image. For this, I took some help from ChatGPT and was informed I could try various filters. It gave me the following list (the item names and text are as provided by ChatGPT):

* **Nearest Neighbor**: The simplest and most straightforward algorithm. Each pixel of the resulting image is the color of the nearest pixel in the original image, creating a blocky and pixelated effect. This algorithm is fast but can result in jagged edges and loss of detail.

* **Box Filter**: This algorithm averages the color values of a block of pixels in the original image and assigns that average color to all the pixels in the corresponding block of the resulting image. It creates a smoother pixelation effect compared to nearest-neighbor, reducing the jagged edges.

* **Scale2x**: Scale2x is a scale-up algorithm that doubles the size of each pixel by replicating the original pixel's color in a 2x2 block. It smooths out the pixelation effect and creates a cleaner look.

* **Pixel Art Shader**: Pixel art shaders simulate the appearance of pixel art by introducing dithering, color palette restrictions, and limited shading. These shaders can be customized to achieve various pixelation styles and effects.

* **Ordered Dithering**: Ordered dithering uses a predefined matrix pattern to selectively adjust the pixel values, simulating additional colors and shades within the limited color palette. It creates the illusion of smoother gradients and a wider range of colors in pixel art.

* **Error Diffusion**: Error diffusion algorithms, such as Floyd-Steinberg or Jarvis-Judice-Ninke, distribute the quantization errors caused by reducing the color palette to neighboring pixels. This diffusion helps maintain the overall visual quality while achieving a pixelated appearance.

This is a lot of things I don't quite understand. By reading these names and descriptions alone I could not figure out how many of them would lead to a pixelation effect. How would nearest-neighbor work if I'm converting a 512x512 image into a 10x10 image? What's the nearest neighbor of pixel 3,2 of the 10x10 image in the 512x512 image? And how would scaling in the Scale2x technique help? Pixel Art Shader seems like an area to explore more but it sounds like the name for a class of algorithms or techniques than a single one and Ordered Dithering and Error Diffusion are both things that go over my head.

The only thing that made sense immediately was the Box Filter. Define a box for a block of pixels and the colour of that box would be the average colour of those pixels. That makes immediate sense. So I decided to try that out first.

For the purpose of experimentation, I chose 4 images:

<img width="200" src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/509fb37e-3207-4652-8bd5-8a311872ceb9"><img width="200" src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/8ef60f77-afe1-4f4f-a92f-52b5845b9161">
<img width="200" src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/57a3d8d1-a81e-4fca-ab07-aae5cc79feb7"><img width="200" src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/e2a7fd6e-06de-4225-b32c-099ea405534a">

I built a pixelation function based on the Box Filter algorithm and got these results:

<img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/68e0016d-2469-4185-ae6e-cabb53c9a2f5"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/615a40da-2aa3-4472-82db-c59f08a6ec8b"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/156b0360-09c8-40bf-a9f2-cb83ec5be4dc"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/eeef5436-e84c-42d7-987e-bbada7268a86">

I...don't like this. While the banana and duck look somewhat recognisable, the elephant is missing a discernible trunk and Shrek's ears are all but a blobby mess. But maybe that's okay if we use 15 pixels instead of 10? 

<img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/c76ea679-71ac-479e-a9cb-d4cc04dd7b41"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/263ffab1-90c7-4c98-8587-9fa3b881ea99"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/e786d78d-770e-4615-b20b-99f9ab5dafda"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/2966399c-e3b0-4b98-831c-521bbf113098">

That's a lot better! The elephant has a defined trunk and Shrek has ears.

Additionally, for generating a Nonogram I will need to turn the pixelated image into a single-tone image. For each pixel, I need to know if it will have a filled or empty block.

Using a step function that cuts off at 0.5, I implemented an elementary single-tone filter (I think there's a better name for it but I don't know what it is). I first applied the Box filter for pixelation and then the single-tone filter.

<img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/dc793f04-369e-47d2-b95c-42d0c62461e8"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/3bb4dcba-0965-4b51-a4a0-178b90388eea"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/a6ee4320-dd1f-4a5f-a42b-be2eb8b03913"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/e3b43b95-9352-43a6-811d-8a55a716abf6">

Oh...oh no!

While the duck and the elephant are recognisable, the banana and Shrek are just smears. This makes sense. Neither has any sort of outline or a significantly dark silhouette. A naive step-function approach won't work here.

This next part of the article is being written several days later. My usual style of writing long-form articles, especially ones that involve a project with multiple steps is to work on a step, record my work and thoughts, rinse, and repeat. I find it helps me solidify my understanding of what I have just done or learned.

Sometimes what I thought would be a single discrete step becomes so large that I have to break it down into further smaller steps and so we are back here. I have not completed the larger step that I wanted to but I have completed a few smaller steps within that larger step. I fear the word "step" is starting to lose all meaning.

I decided to look for nice pixelation algorithms, preferably something that took a context-aware approach, that is to say, something that kept in mind that a pixelated image still had to look somewhat like the original, especially in conserving outlines and critical features.

It led me to this research paper[Pixelated Image Abstraction by Gerstner et al.](https://gfx.cs.princeton.edu/pubs/Gerstner_2012_PIA/Gerstner_2012_PIA_small.pdf). The concepts explained within are wildly out of my reach of understanding. I do not have the necessary mathematical or computer science background. But what I do have is some minor talent in the [Art of the Bodge](https://youtu.be/lIFE7h3m40U). I can cobble together things without fully understanding how they work.

Reading through the research paper I came to understand that I would have to figure out a few distinct pieces:
* Converting Unity's [Color](https://docs.unity3d.com/ScriptReference/Color.html)(which uses RGB) to a [CIELAB](https://nl.wikipedia.org/wiki/CIELAB) color space.
* Figure out the Critical Temperature (see section 4.1 in the paper) by performing [Principal Component Analysis](https://builtin.com/data-science/step-step-explanation-principal-component-analysis).
* Write a modified SLIC algorithm based on the research paper.

  ...and a few other things I haven't gotten to yet.

For the PCA I'm using [Mathnet.Numerics](https://numerics.mathdotnet.com/) and for the CIELAB color space I'm making heavy use of code from [Patrick Wu's article on color conversions in C#](https://patrickwu.space/2016/06/12/csharp-color/).

I started off by applying [SLIC](https://www.iro.umontreal.ca/~mignotte/IFT6150/Articles/SLIC_Superpixels.pdf). I thought it would be a good idea to get a handle on the base SLIC algorithm before trying out the modifications used in the Gerstner paper. This is what it looks like after applying SLIC and then setting the color of each superpixel to the mean color of its component pixels.

<img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/ba06b37b-1fd0-4b35-879c-e8bdb977055d"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/e44370a0-1425-4f44-b760-1fe120509363"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/fbe432ab-80c9-44f5-905e-14b9249257f0"><img width=200 src="https://github.com/BadgerBadgerBadgerBadger/BadgerBadgerBadgerBadger.github.io/assets/5138570/89579b01-2fdf-4748-a536-1d7ba74403d8">

This looks like...something. But it runs very slowly. Each pixelation process takes almost 10 seconds. And while this isn't meant to be used in a realtime pipeline, 10 seconds is still too long. It is then that I started looking into the [Unity Job System](https://www.kodeco.com/7880445-unity-job-system-and-burst-compiler-getting-started).

I implemented a first version and...it made things worse. The 10 seconds became 25 seconds 🤦
